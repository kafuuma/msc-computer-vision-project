{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRn67sI_bzrm"
   },
   "source": [
    "<h1> Training Classifiers and performance assessment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gc2wn_6IBun6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import sys, os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2HbJKLRbzrr"
   },
   "source": [
    "### Retrieving and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 424,
     "status": "error",
     "timestamp": 1574678666514,
     "user": {
      "displayName": "Henrik Høiness",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU_UzKC8CKFaMzdXHG3v2miAUiqbdhU_utY4l9=s64",
      "userId": "05134007726078583058"
     },
     "user_tz": 480
    },
    "id": "4fkkK5APDZRb",
    "outputId": "87d21858-b8c7-461c-8982-b0b2564fb7a5"
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.feature_extraction_and_exploratory_data_analysis import get_preprocessed_feature_frame\n",
    "from ipynb.fs.defs.transfer_learning_cnn import get_recall, get_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 421,
     "status": "error",
     "timestamp": 1574675788370,
     "user": {
      "displayName": "Henrik Høiness",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU_UzKC8CKFaMzdXHG3v2miAUiqbdhU_utY4l9=s64",
      "userId": "05134007726078583058"
     },
     "user_tz": 480
    },
    "id": "B7Qeocn8Beni",
    "outputId": "c1019e4b-4e96-4ee7-e30b-18bb86dd68a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing data:\n",
    "#  • Seperating data into train, validation and test data.\n",
    "#     - 'training_data' is data used for hypertuning\n",
    "#\n",
    "#  • We will use 'validation_data' as our own test data, to evaluate the final models.\n",
    "#     - 'validation_data' will be held aside until all hypertuning of both preprocessing parameters and model parameters is done.\n",
    "#     - 'test_data' refers to the unlabeled images, which we will use our best model to predict labels for, and submitting these predictions.\n",
    "\n",
    "# Preprocessing variables below is arbitrary, and only represents a default setting for our preprocessing parameters\n",
    "downsample, k, desc_limit = False, 2, 1000\n",
    "\n",
    "# This specifies the train/val split\n",
    "validation_size = 0.1\n",
    "\n",
    "training_data, validation_data, test_data = get_preprocessed_feature_frame(k=k, desc_limit=desc_limit, downsample=downsample, validation_size=validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "print(f\"Validation data shape: {validation_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the PCA algorithm with our data, did not give promising results, therefore not using in final version of classification\n",
    "\n",
    "def show_variance_explained_pca(data):\n",
    "    \"\"\"\n",
    "    Showing cumulative variance explained by the principle components after performing pca on input dataframe\n",
    "    \"\"\"\n",
    "    pca = PCA().fit(data)\n",
    "    #Plotting the Cumulative Summation of the Explained Variance\n",
    "    plt.figure()\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance (%)') #for each component\n",
    "    plt.title('Explained Variance')\n",
    "    plt.show()\n",
    "    \n",
    "def fit_pca(pca, train_x, val_x):\n",
    "    \"\"\"\n",
    "    Returning the transformed datasets using the provided pca\n",
    "    Fitting using only training data, transforms both\n",
    "    \"\"\"\n",
    "    train_x_pca = pca.fit_transform(train_x)\n",
    "    val_x_pca = pca.transform(val_x)    \n",
    "    return train_x_pca, test_x_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing 5-fold cross validation to find optimal preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual_y, predicted_y):\n",
    "    \"\"\"\n",
    "    The root mean square error between the prediction and the ground truth\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((actual_y - predicted_y)**2)/len(predicted_y))\n",
    "\n",
    "\n",
    "def compute_CV_error_acc_rec_prec(model, X_train, Y_train):\n",
    "    '''\n",
    "    Split the training data into 5 subsets.\n",
    "    For each subset:\n",
    "        - Fit a model holding out that subset\n",
    "        - Compute the RMSE on that subset (the validation set)\n",
    "\n",
    "    @param models: List of sklearn models with fit and predict functions \n",
    "    @param X_train: X training data\n",
    "    @param Y_train: Y training data\n",
    "    \n",
    "    @return: The average RMSE, Accuracy, Recall and Precision of these 5 folds.\n",
    "    '''\n",
    "    kf = KFold(n_splits=5, random_state=42)\n",
    "    validation_errors = []\n",
    "    validation_accuracies = []\n",
    "    validation_recall = []\n",
    "    validation_precision = []\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        \n",
    "        # Split the data\n",
    "        split_X_train, split_X_valid = np.take(X_train, train_idx, axis=0), np.take(X_train, valid_idx, axis=0)\n",
    "        split_Y_train, split_Y_valid = np.take(Y_train, train_idx, axis=0), np.take(Y_train, valid_idx, axis=0)\n",
    "        \n",
    "        # Fit the model on the training split\n",
    "        model.fit(split_X_train, split_Y_train)\n",
    "        \n",
    "        # Compute the RMSE on the validation split\n",
    "        preds = model.predict(split_X_valid)\n",
    "        \n",
    "        # Computing metrics\n",
    "        error = rmse(split_Y_valid, preds)\n",
    "        acc = accuracy_score(split_Y_valid, preds)\n",
    "        recall = np.mean(get_recall(preds, split_Y_valid))\n",
    "        precision = np.mean(get_precision(preds, split_Y_valid))\n",
    "        \n",
    "        validation_errors.append(error)\n",
    "        validation_accuracies.append(acc)\n",
    "        validation_recall.append(recall)\n",
    "        validation_precision.append(precision)\n",
    "        \n",
    "    return np.mean(validation_errors), np.mean(validation_accuracies), np.mean(validation_recall), np.mean(validation_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "def compute_multimodel_CV_rmse_and_acc(models, training_df, k, down_sample, desc_limit):\n",
    "    '''\n",
    "    Split the training data into 5 subsets.\n",
    "    For each subset:\n",
    "        - Fit a model holding out that subset\n",
    "        - Compute the RMSE on that subset (the validation set)\n",
    "        \n",
    "    You should be fitting 5 models total.\n",
    "\n",
    "    @param models: List of sklearn models with fit and predict functions \n",
    "    @param training_df: Training dataframe: Training data\n",
    "    @param k: K for KMeans\n",
    "    @param down_sample: Whether or not to down-sample\n",
    "    @param desc_limit: Descriptor limit\n",
    "    \n",
    "    @return: The average RMSE, Accuracy, Recall and Precision of these 5 folds.\n",
    "    '''\n",
    "    kf = KFold(n_splits=5, random_state=42)\n",
    "    validation_accuracies = [[] for _ in range(len(models))]\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(training_df):\n",
    "        \n",
    "        # Split the data\n",
    "        split_train = training_df.iloc[train_idx]\n",
    "        split_valid = training_df.iloc[valid_idx]\n",
    "        \n",
    "        train, _, val = get_preprocessed_feature_frame(split_train, split_valid, validation_size=0.0, k=k, desc_limit=desc_limit)\n",
    "        \n",
    "        train_x, train_y = train.drop(columns=['Label', 'Image_Id', 'Scaled_Image', 'Image']), train['Label']\n",
    "        val_x, val_y = val.drop(columns=['Label', 'Image_Id', 'Scaled_Image', 'Image']), val['Label']\n",
    "        \n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        train_x_scaled = scaler.fit_transform(train_x)\n",
    "        val_x_scaled = scaler.transform(val_x)\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            model.fit(train_x_scaled, train_y)\n",
    "            preds = model.predict(val_x_scaled)\n",
    "\n",
    "            acc = accuracy_score(preds, val_y)\n",
    "            validation_accuracies[i].append(acc)\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        print(type(models[i]))\n",
    "        print(f\"Cross validation mean accuracy: {np.mean(validation_accuracies[i])}\")\n",
    "        \n",
    "    return validation_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for finding preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_preprocessing_parameters(train_data):\n",
    "    \"\"\"\n",
    "    Performing gridsearch with five-fold cross-validation to find optimal preprocessing for all classifiers.\n",
    "    \"\"\"\n",
    "    accs = []\n",
    "    for k in [10, 50, 100]:\n",
    "        for down_sample in [True, False]:\n",
    "            for descriptor_limit in [5000, 10000, 12000, 30000]:\n",
    "                \n",
    "                print(f'**** k={k}, down_sample={down_sample}, decriptor_limit={descriptor_limit} ****')\n",
    "                \n",
    "                \n",
    "                models = [LogisticRegression(max_iter=1000), \n",
    "                          KNeighborsClassifier(10, weights='distance'),\n",
    "                          DecisionTreeClassifier(),\n",
    "                          RandomForestClassifier(n_estimators=800),\n",
    "                          SVC(kernel='rbf',C=10, gamma=0.01)]\n",
    "                data = train_data.copy()[['Image_Id', 'Image', 'Label']]\n",
    "                accs.append(compute_multimodel_CV_rmse_and_acc(models, data, k, down_sample, descriptor_limit))\n",
    "    \n",
    "    return accs\n",
    "                \n",
    "                \n",
    "#grid_search_preprocessing_parameters(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from grid-search for optimal preprocessing\n",
    "\n",
    "After performing grid search to find optimal preprocessing parameters on base models (without tuned hyper parameters), we have the following results:\n",
    "\n",
    "| Model               | K   | Under-sampling | Descriptor limit | Achieved CV Acc |\n",
    "|---------------------|-----|----------------|------------------|-----------------|\n",
    "| Logistic Regression | 100 | True           | 12,000           | 0.440           |\n",
    "| k-NN                | 100 | False          | 12,000           | 0.334           |\n",
    "| Decision Tree       | 100 | False          | 5,000            | 0.301           |\n",
    "| Random Forest       | 100 | False          | 12,000           | 0.466           |\n",
    "| SVM                 | 100 | False          | 5,000            | 0.423           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters for classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_hyperparameters(base_model, random_grid, train_x, train_y):\n",
    "    \"\"\"\n",
    "    Performing a randomized grid search with 3-fold cross validation to find optimal hyperparameters for given model.\n",
    "    Random grid contains the list of parameters to try.\n",
    "    \"\"\"\n",
    "    model_random = RandomizedSearchCV(estimator = base_model, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=1, random_state=42, n_jobs = -1)\n",
    "    model_random.fit(train_x, train_y)\n",
    "    return model_random\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(k, desc_limit, downsample):\n",
    "    \"\"\"\n",
    "    Returning training and validation data, given a set of preprocessing parameters.\n",
    "    The validation data will always contain the same images.\n",
    "    \"\"\"\n",
    "    train_data, val_data, _ = get_preprocessed_feature_frame(k=k, desc_limit=desc_limit, downsample=downsample, validation_size=0.1)\n",
    "    train_x, train_y = train_data.drop(columns=['Label', 'Image_Id', 'Scaled_Image', 'Image']), train_data['Label']\n",
    "    val_x, val_y = val_data.drop(columns=['Label', 'Image_Id', 'Scaled_Image', 'Image']), val_data['Label']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_x_scaled = scaler.fit_transform(train_x)\n",
    "    val_x_scaled = scaler.transform(val_x)\n",
    "    \n",
    "    return train_x_scaled, train_y, val_x_scaled, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPgsvYbhbzrs"
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Using preprocessing parameters:\n",
    "* K: 100\n",
    "* Down-sample: True\n",
    "* Descriptor limit: 12,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Beginning preprocessing part one\n",
      "[INFO] Reading training images\n",
      "\t [100.0 %] Fetching label 'zebra'                 \n",
      "[INFO] Reading testing images\n",
      "\t [100.0 %] Fetching 716 images'\n",
      "[INFO] Downsampling training images..\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Getting descriptors\n",
      "[INFO] Fitting KMeans with k=100 to training descriptors\n",
      "[INFO] Adding cluster features\n",
      "[INFO] Done preprocessing. Total time elapsed: 1162.864945711\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = get_train_val_data(k=100, desc_limit=12000, downsample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.2min finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l2']\n",
    "solver = ['lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "C = np.logspace(-4, 4, 20)\n",
    "multi_class = ['multinomial', 'ovr']\n",
    "class_weight = [None, 'balanced']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'penalty': penalty,\n",
    "               'solver': solver,\n",
    "               'C': C,\n",
    "               'multi_class': multi_class,\n",
    "               'class_weight': class_weight}\n",
    "\n",
    "base_model = LogisticRegression()\n",
    "logreg_optimal = get_optimal_hyperparameters(base_model, random_grid, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for model:\n",
      "{'solver': 'saga', 'penalty': 'l2', 'multi_class': 'multinomial', 'class_weight': None, 'C': 29.763514416313132}\n",
      "Mean cross-validated score of the best_estimator: 0.44076655052264807\n"
     ]
    }
   ],
   "source": [
    "print(f'Optimal parameters for model:\\n{logreg_optimal.best_params_}')\n",
    "print(f'Mean cross-validated score of the best_estimator: {logreg_optimal.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model with  5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1074,
     "status": "error",
     "timestamp": 1574642755220,
     "user": {
      "displayName": "Axel Oevreboe Harstad",
      "photoUrl": "",
      "userId": "00313732855330319553"
     },
     "user_tz": 480
    },
    "id": "p_F4f5Slbzrt",
    "outputId": "4d14a70d-77fb-43e6-98f0-24c5cb84814c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 6.071572810656048\n",
      "Accuracy: 0.43901651794190244\n",
      "Recall: 0.43155606007208547\n",
      "Precision: 0.418413309187222\n"
     ]
    }
   ],
   "source": [
    "logreg_optimal_model = logreg_optimal.best_estimator_\n",
    "error, acc, recall, prec = compute_CV_error_acc_rec_prec(logreg_optimal_model, train_x, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {prec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model on separate validation (\"test\") set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.3984375\n",
      "Recall: 0.37735750295717374\n",
      "Precision: 0.3948733203352865\n"
     ]
    }
   ],
   "source": [
    "logreg_optimal_model.fit(train_x, train_y)\n",
    "preds = logreg_optimal_model.predict(val_x)\n",
    "acc = accuracy_score(preds, val_y)\n",
    "recall = np.mean(get_recall(preds, val_y))\n",
    "precision = np.mean(get_precision(preds, val_y))\n",
    "\n",
    "print(f\"Test accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcCxllL3bzrv"
   },
   "source": [
    "### K-nearest Neighbors\n",
    "\n",
    "Using preprocessing parameters:\n",
    "* K: 100\n",
    "* Down-sample: False\n",
    "* Descriptor limit: 12,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Beginning preprocessing part one\n",
      "[INFO] Reading training images\n",
      "\t [100.0 %] Fetching label 'zebra'                 \n",
      "[INFO] Reading testing images\n",
      "\t [100.0 %] Fetching 716 images'\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Getting descriptors\n",
      "[INFO] Fitting KMeans with k=100 to training descriptors\n",
      "[INFO] Adding cluster features\n",
      "[INFO] Done preprocessing. Total time elapsed: 1161.3637902209998\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = get_train_val_data(k=100, desc_limit=12000, downsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.8min finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "n_neighbors=[i for i in range(1, 30, 2)]\n",
    "weights = ['distance', 'uniform']\n",
    "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "p = [i for i in range(1, 5)]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_neighbors': n_neighbors,\n",
    "               'weights': weights,\n",
    "               'algorithm': algorithm,\n",
    "               'p': p}\n",
    "\n",
    "base_model = KNeighborsClassifier()\n",
    "knn_optimal = get_optimal_hyperparameters(base_model, random_grid, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for model:\n",
      "{'weights': 'distance', 'p': 1, 'n_neighbors': 27, 'algorithm': 'auto'}\n",
      "Mean cross-validated score of the best_estimator: 0.38296296296296295\n"
     ]
    }
   ],
   "source": [
    "print(f'Optimal parameters for model:\\n{knn_optimal.best_params_}')\n",
    "print(f'Mean cross-validated score of the best_estimator: {knn_optimal.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model with  5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLQdVbRAbzrw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 6.104989021357785\n",
      "Accuracy: 0.37851851851851853\n",
      "Recall: 0.32909701414022774\n",
      "Precision: 0.3955269805386118\n"
     ]
    }
   ],
   "source": [
    "knn_optimal_model = knn_optimal.best_estimator_\n",
    "error, acc, recall, prec = compute_CV_error_acc_rec_prec(knn_optimal_model, train_x, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {prec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model on separate validation (\"test\") set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.33112582781456956\n",
      "Recall: 0.3097817454772094\n",
      "Precision: 0.34377966839951635\n"
     ]
    }
   ],
   "source": [
    "knn_optimal_model.fit(train_x, train_y)\n",
    "preds = knn_optimal_model.predict(val_x)\n",
    "acc = accuracy_score(preds, val_y)\n",
    "recall = np.mean(get_recall(preds, val_y))\n",
    "precision = np.mean(get_precision(preds, val_y))\n",
    "\n",
    "print(f\"Test accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eyf9J_TLbzry"
   },
   "source": [
    "### Classification Tree\n",
    "\n",
    "Using preprocessing parameters:\n",
    "* K: 100\n",
    "* Down-sample: False\n",
    "* Descriptor limit: 5,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Beginning preprocessing part one\n",
      "[INFO] Reading training images\n",
      "\t [100.0 %] Fetching label 'zebra'                 \n",
      "[INFO] Reading testing images\n",
      "\t [100.0 %] Fetching 716 images'\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Getting descriptors\n",
      "[INFO] Fitting KMeans with k=100 to training descriptors\n",
      "[INFO] Adding cluster features\n",
      "[INFO] Done preprocessing. Total time elapsed: 444.4378227330003\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = get_train_val_data(k=100, desc_limit=5000, downsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPQlFzuebzry"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    3.4s finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "criterion=['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'criterion': criterion,\n",
    "               'splitter': splitter,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "base_model = DecisionTreeClassifier()\n",
    "dt_optimal = get_optimal_hyperparameters(base_model, random_grid, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for model:\n",
      "{'splitter': 'best', 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'gini'}\n",
      "Mean cross-validated score of the best_estimator: 0.2659259259259259\n"
     ]
    }
   ],
   "source": [
    "print(f'Optimal parameters for model:\\n{dt_optimal.best_params_}')\n",
    "print(f'Mean cross-validated score of the best_estimator: {dt_optimal.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model with  5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 6.93302199941984\n",
      "Accuracy: 0.23851851851851852\n",
      "Recall: 0.20386586244493232\n",
      "Precision: 0.2094802008063556\n"
     ]
    }
   ],
   "source": [
    "dt_optimal_model = dt_optimal.best_estimator_\n",
    "error, acc, recall, prec = compute_CV_error_acc_rec_prec(dt_optimal_model, train_x, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {prec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model on separate validation (\"test\") set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.25165562913907286\n",
      "Recall: 0.2577579359298161\n",
      "Precision: 0.22700396792187982\n"
     ]
    }
   ],
   "source": [
    "dt_optimal_model.fit(train_x, train_y)\n",
    "preds = dt_optimal_model.predict(val_x)\n",
    "acc = accuracy_score(preds, val_y)\n",
    "recall = np.mean(get_recall(preds, val_y))\n",
    "precision = np.mean(get_precision(preds, val_y))\n",
    "\n",
    "print(f\"Test accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_T-JIq8bzr1"
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "Using preprocessing parameters:\n",
    "* K: 100\n",
    "* Down-sample: False\n",
    "* Descriptor limit: 12,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Beginning preprocessing part one\n",
      "[INFO] Reading training images\n",
      "\t [100.0 %] Fetching label 'zebra'                 \n",
      "[INFO] Reading testing images\n",
      "\t [100.0 %] Fetching 716 images'\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Getting descriptors\n",
      "[INFO] Fitting KMeans with k=100 to training descriptors\n",
      "[INFO] Adding cluster features\n",
      "[INFO] Done preprocessing. Total time elapsed: 1471.2321443980036\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = get_train_val_data(k=100, desc_limit=12000, downsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 19.1min finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] \n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] \n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "base_model = RandomForestClassifier()\n",
    "rf_optimal = get_optimal_hyperparameters(base_model, random_grid, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for model:\n",
      "{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': False}\n",
      "Mean cross-validated score of the best_estimator: 0.48148148148148145\n"
     ]
    }
   ],
   "source": [
    "print(f'Optimal parameters for model:\\n{rf_optimal.best_params_}')\n",
    "print(f'Mean cross-validated score of the best_estimator: {rf_optimal.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model with  5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 5.430012725761354\n",
      "Accuracy: 0.47703703703703704\n",
      "Recall: 0.43209351177628885\n",
      "Precision: 0.4864141379478788\n"
     ]
    }
   ],
   "source": [
    "rf_optimal_model = rf_optimal.best_estimator_\n",
    "error, acc, recall, prec = compute_CV_error_acc_rec_prec(rf_optimal_model, train_x, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {prec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model on separate validation (\"test\") set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5231788079470199\n",
      "Recall: 0.5106746021792027\n",
      "Precision: 0.5226237144380881\n"
     ]
    }
   ],
   "source": [
    "rf_optimal_model.fit(train_x, train_y)\n",
    "preds = rf_optimal_model.predict(val_x)\n",
    "acc = accuracy_score(preds, val_y)\n",
    "recall = np.mean(get_recall(preds, val_y))\n",
    "precision = np.mean(get_precision(preds, val_y))\n",
    "\n",
    "print(f\"Test accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "Using preprocessing parameters:\n",
    "* K: 100\n",
    "* Down-sample: False\n",
    "* Descriptor limit: 5,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Beginning preprocessing part one\n",
      "[INFO] Reading training images\n",
      "\t [100.0 %] Fetching label 'zebra'                 \n",
      "[INFO] Reading testing images\n",
      "\t [100.0 %] Fetching 716 images'\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Getting descriptors\n",
      "[INFO] Fitting KMeans with k=100 to training descriptors\n",
      "[INFO] Adding cluster features\n",
      "[INFO] Done preprocessing. Total time elapsed: 470.1122136030026\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = get_train_val_data(k=100, desc_limit=5000, downsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.4min finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "C = np.logspace(-4, 4, 20)\n",
    "kernel = ['rbf', 'linear', 'poly', 'sigmoid'] \n",
    "gamma = ['scale', 'auto', 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "shrinking = [True, False]\n",
    "class_weight = [None,'balanced']\n",
    "decision_function_shape = ['ovo', 'ovr']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'C': C,\n",
    "               'kernel': kernel,\n",
    "               'gamma': gamma,\n",
    "               'shrinking': shrinking,\n",
    "               'class_weight': class_weight,\n",
    "               'decision_function_shape': decision_function_shape}\n",
    "\n",
    "base_model = SVC()\n",
    "svc_optimal = get_optimal_hyperparameters(base_model, random_grid, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters for model:\n",
      "{'shrinking': False, 'kernel': 'linear', 'gamma': 'auto', 'decision_function_shape': 'ovo', 'class_weight': None, 'C': 0.004832930238571752}\n",
      "Mean cross-validated score of the best_estimator: 0.45185185185185184\n"
     ]
    }
   ],
   "source": [
    "print(f'Optimal parameters for model:\\n{svc_optimal.best_params_}')\n",
    "print(f'Mean cross-validated score of the best_estimator: {svc_optimal.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model with  5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 5.743792808403207\n",
      "Accuracy: 0.43851851851851853\n",
      "Recall: 0.3949599648185278\n",
      "Precision: 0.4400294290263521\n"
     ]
    }
   ],
   "source": [
    "svc_optimal_model = svc_optimal.best_estimator_\n",
    "error, acc, recall, prec = compute_CV_error_acc_rec_prec(svc_optimal_model, train_x, train_y)\n",
    "print(f\"Mean RMSE: {error}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {prec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating optimal model on separate validation (\"test\") set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.45695364238410596\n",
      "Recall: 0.45248015787922197\n",
      "Precision: 0.4513556610752848\n"
     ]
    }
   ],
   "source": [
    "svc_optimal_model.fit(train_x, train_y)\n",
    "preds = svc_optimal_model.predict(val_x)\n",
    "acc = accuracy_score(preds, val_y)\n",
    "recall = np.mean(get_recall(preds, val_y))\n",
    "precision = np.mean(get_precision(preds, val_y))\n",
    "\n",
    "print(f\"Test accuracy: {acc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting provided test set\n",
    "\n",
    "Here we are predicting the unlabeled testing set, provided with the assignment.\n",
    "We will be predicting using our RandomForest model with tuned hyperparameters, which performed best with an cross-validation accuracy of $0.477$ and test accuracy of $0.523$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Beginning preprocessing part one\n",
      "[INFO] Reading training images\n",
      "\t [100.0 %] Fetching label 'zebra'                 \n",
      "[INFO] Reading testing images\n",
      "\t [100.0 %] Fetching 716 images'\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Denoising images\n",
      "[INFO] Trimming images\n",
      "[INFO] Scaling images\n",
      "[INFO] Calculating mean and standard deviation for HOG feature\n",
      "[INFO] Calculating mean of Harris Corner response map\n",
      "[INFO] Adding features: size, aspect_ratio, red-, green-, blue- and gray-intensity\n",
      "[INFO] Adding color histogram features\n",
      "[INFO] Adding stride features\n",
      "[INFO] Getting descriptors\n",
      "[INFO] Fitting KMeans with k=100 to training descriptors\n",
      "[INFO] Adding cluster features\n",
      "[INFO] Done preprocessing. Total time elapsed: 1328.0627330140014\n"
     ]
    }
   ],
   "source": [
    "# Using preprocessing parameters that, found through cross-validation, is most optimal for our best performing model\n",
    "k, desc_limit, downsample, validation_size = 100, 12000, False, 0.0\n",
    "training_data, _, test_data = get_preprocessed_feature_frame(k=k, desc_limit=desc_limit, downsample=downsample, validation_size=validation_size,  data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1501, 676)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Training set shape: {training_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = training_data.drop(columns=['Label', 'Image_Id', 'Scaled_Image', 'Image']), training_data['Label']\n",
    "test_x = test_data.drop(columns=['Image_Id', 'Scaled_Image', 'Image'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x)\n",
    "test_x_scaled = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf_optimal_model\n",
    "model.fit(train_x_scaled, train_y)\n",
    "\n",
    "preds = model.predict(test_x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label\n",
       "0     14\n",
       "1      8\n",
       "2     14\n",
       "3      3\n",
       "4      3\n",
       "5      6\n",
       "6     14\n",
       "7      9\n",
       "8     14\n",
       "9      9"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame(preds, columns=[\"Label\"])\n",
    "prediction_df.to_csv('predictions.csv', header=False, index=False)\n",
    "\n",
    "prediction_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GradProject_NB3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
